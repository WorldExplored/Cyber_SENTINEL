import torch
import torch.nn as nn
import torch.optim as optim

class ALOHAModel(nn.Module):
    def __init__(self, input_dim):
        super(ALOHAModel, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        return self.network(x)
    
import sqlite3
import pandas as pd


# Connect to the SQLite database/content/
conn = sqlite3.connect('meta.db')

# Query to list all tables in the database
table_query = "SELECT name FROM sqlite_master WHERE type='table';"
tables = pd.read_sql_query(table_query, conn)

# Assuming you want to sample from the first table
if not tables.empty:
    table_name = tables['name'].iloc[0]

    # Define the number of rows to sample
    num_rows_to_sample = 300000  # Adjust this number based on your RAM capacity

    # Query to retrieve a subset of rows
    data_query = f"SELECT * FROM {table_name} LIMIT {num_rows_to_sample};"
    data_subset = pd.read_sql_query(data_query, conn)
    print(f"Loaded {len(data_subset)} rows from the table '{table_name}'.")
else:
    print("No tables found in the database.")

# Close the database connection
conn.close()


from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader, TensorDataset

# Ensure that 'features_to_drop' only contains the columns you want to remove
features_to_drop = ['rl_ls_const_positives']

# Confirm the list of columns to drop
print("Columns to drop:", features_to_drop)

# Drop the columns and confirm the remaining columns
X = data_subset.drop(features_to_drop, axis=1)
remaining_features = X.columns.tolist()
print("Remaining features:", remaining_features)

# Check if 'is_malware' is still in the DataFrame and remove it if so
if 'is_malware' in X.columns:
    y = data_subset['is_malware']
    X.drop(['is_malware'], axis=1, inplace=True)
else:
    raise ValueError("'is_malware' column is not found. Check the DataFrame.")

# Now check if X has any columns left
if X.shape[1] == 0:
    raise ValueError("No features left for training. Check the feature dropping process.")

# Assuming 'features_to_drop' and data preparation steps have been correctly done
# Drop the 'sha256' column along with other specified features and the target variable from X
features_to_drop.append('sha256')  # Ensure 'sha256' is in the list of features to drop
X = data_subset.drop(features_to_drop + ['is_malware'], axis=1)

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Convert X and y to PyTorch tensors
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

# Create DataLoader instances
train_data = TensorDataset(X_train_tensor, y_train_tensor)
test_data = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = DataLoader(test_data, batch_size=64, shuffle=False)


# Initialize the model
input_dim = X_train_tensor.shape[1]
model = ALOHAModel(input_dim)

# Define loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
epochs = 10
for epoch in range(epochs):
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        output = model(inputs)
        loss = criterion(output, labels)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}")


model.eval()
with torch.no_grad():
    correct = 0
    total = 0
    for inputs, labels in test_loader:
        output = model(inputs)
        predicted = (output > 0.5).float()
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
print(f"Accuracy: {correct / total}")











from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np

# Assuming model is already trained and has multiple outputs, one for each tag
# Assuming you have a list of tag names in the same order as the model outputs
tag = 'ransomware_tag' #, 'flooder_tag', 'ransomware_tag']  # Fill in with actual tag names

# Evaluate the model and collect the predictions
model.eval()
tag_prediction = {tag: []}  # Dictionary to store predictions for each tag
all_labels = []  # List to store all true labels

with torch.no_grad():
    for inputs, labels in test_loader:
        outputs = model(inputs)  # Assume the model outputs a tensor with one column per tag
        all_labels.extend(labels.numpy())
        
        # For single-label classification, there is no need to index outputs
        tag_prediction[tag].extend(outputs.numpy())


# Calculate ROC AUC for each tag and plot ROC curve
plt.figure(figsize=(10, 8))

tag_fpr, tag_tpr, _ = roc_curve(all_labels, tag_prediction[tag])
tag_roc_auc = auc(tag_fpr, tag_tpr)
    
    # Plot ROC curve for the tag
plt.plot(tag_fpr, tag_tpr, lw=2, label=f'{tag} (area = {tag_roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')
plt.xscale('log')
plt.xlim([10**-6, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic for Multiple Tags')
plt.legend(loc="lower right")
plt.show()
