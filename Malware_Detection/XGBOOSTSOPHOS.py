import sqlite3
import pandas as pd
from sklearn.metrics import roc_auc_score, roc_curve, auc
import matplotlib.pyplot as plt

# Connect to the SQLite database
conn = sqlite3.connect('meta.db')

# Query to list all tables in the database
table_query = "SELECT name FROM sqlite_master WHERE type='table';"
tables = pd.read_sql_query(table_query, conn)

# Assuming you want to sample from the first table
if not tables.empty:
    table_name = tables['name'].iloc[0]
    
    # Define the number of rows to sample
    num_rows_to_sample = 10000  # Adjust this number based on your RAM capacity

    # Query to retrieve a subset of rows
    data_query = f"SELECT * FROM {table_name} LIMIT {num_rows_to_sample};"
    data_subset = pd.read_sql_query(data_query, conn)
    print(f"Loaded {len(data_subset)} rows from the table '{table_name}'.")
else:
    print("No tables found in the database.")

# Close the database connection
conn.close()


import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Ensure that 'features_to_drop' only contains the columns you want to remove
features_to_drop = ['rl_ls_const_positives']

# Confirm the list of columns to drop
print("Columns to drop:", features_to_drop)

# Drop the columns and confirm the remaining columns
X = data_subset.drop(features_to_drop, axis=1)
remaining_features = X.columns.tolist()
print("Remaining features:", remaining_features)

# Check if 'is_malware' is still in the DataFrame and remove it if so
if 'is_malware' in X.columns:
    y = data_subset['is_malware']
    X.drop(['is_malware'], axis=1, inplace=True)
else:
    raise ValueError("'is_malware' column is not found. Check the DataFrame.")

# Now check if X has any columns left
if X.shape[1] == 0:
    raise ValueError("No features left for training. Check the feature dropping process.")

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Assuming 'features_to_drop' and data preparation steps have been correctly done
# Drop the 'sha256' column along with other specified features and the target variable from X
features_to_drop.append('sha256')  # Ensure 'sha256' is in the list of features to drop
X = data_subset.drop(features_to_drop + ['is_malware'], axis=1)

# Now, 'X' should only contain columns with data types compatible with XGBoost
# Proceed with the rest of the data preparation and model training steps

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the XGBoost classifier with the 'enable_categorical' parameter if you have categorical features
xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', enable_categorical=True)

# Train the classifier with the training data
xgb_clf.fit(X_train, y_train)


# Now, you can make predictions on the test set without encountering the NameError
y_pred = xgb_clf.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy}")

# Generate a classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))
